\chapter{Stray topics}

Ah, you thought this was organized documentation? No, it's generally
written on a laptop, late at night, with a good bottle of wine beside
me (occasionally not so good; every month I put myself in the hands of
the usually stellar experts at the local Wine Merchant), and nothing
better to do. Here's where you can find topics that rank as frequently
asked questions about HMMER.

\section{Evaluating the significance of profile HMM hit}

\prog{hmmsearch} gives you a ranked list of hits in a sequence
database.  Which ones are likely to be true homologous and which ones
are likely to be nonhomologous to your query HMM? (The same
question of course arises in evaluating the significance of
\prog{hmmpfam} hits.)

HMMER gives you at least two scoring criteria to judge by: the HMMER
raw score, and an E-value. Additionally, Pfam models carry a third set
of criteria: six expert-calibrated raw score cutoffs that the Pfam
database maintainers set.

\subsection{Executive summary}

\begin{itemize}

\item The best criterion of statistical significance is the E-value.
The E-value is calculated from the raw score. It tells you how many
false positives you would have expected to see at or above this raw
score. Therefore a low E-value is best; an E-value of 0.1, for
instance, means that there's only a 10\% chance that you would've seen
a hit this good in a search of nonhomologous sequences. {\em
Typically, I trust the results of HMMER searches at about E=0.1 and
below, and I examine the hits manually down to E=10 or so.}

\item A HMMER raw score above $\log_2$ of the number of sequences
in the target database is likely to be a true homologue. For current
NR databases, this rule-of-thumb number is on the order of 20 bits.
Whereas the E-value measures how statistically significant the raw
score is, the raw score itself is telling you how well the sequence
matches your HMM. Because these things should be strongly correlated,
usually, true homologues will have both a good raw score and a good
E-value. However, sometimes (and these are the interesting cases), you
will find remote homologues which do not match the model well (and so
do not have good raw scores -- possibly even negative), but which
nonetheless have significant E-values, indicating that the raw score,
though ``bad'', is still strongly suggestive of homology.

\item For Pfam HMMs, you can also examine six other numbers that
represent raw score thresholds: two TC (trusted cutoff) scores, two GA
(gathering) scores, and two NC (noise cutoff) scores. The meaning of
these numbers is described below.

\subsection{In more detail: What HMMER raw scores mean}

The raw score is a log-odds score in log base two (thus, in units of
{\em bits}. Specifically, it is:

\[
	S = \log_2 \frac {P( \mbox{seq} | \mbox{HMM})} { P (\mbox{seq} |
	\mbox{null})}.
\]

$P( \mbox{seq} | \mbox{HMM}$ is the probability of the target sequence
according to your HMM. $ P (\mbox{seq} | \mbox{null}) $ is the
probability of the target sequence given a ``null hypothesis'' model
of the statistics of random sequence. In HMMER, this null model is a
simple one state HMM that says that random sequences are i.i.d.
sequences with a specific residue composition (this ``null model
distribution'' is part of the HMM save file, and it can be altered
when you do an \prog{hmmbuild}).

Thus, a positive score means the HMM is a better model of the target
sequence than the null model is (e.g. the HMM gives a higher
probability).

\prog{hmmsearch} reports two hit lists, with different types of
scores. The first list is ranked by {\em per-sequence} scores, and the
second is ranked by {\em per-domain} scores. The per-sequence score is
the score of the entire target sequence according to the HMM. A
per-domain score is a score of one matching subsequence (domain) from
the target sequence: the per-domain score is calculated as if that
subsequence was the entire target sequence. 

Because HMMER's Plan 7 model is capable of looping, and therefore of
modeling more than one copy of a particular domain in a target
sequence (for instance, a closely spaced array of immunoglobulin
superfamily domains), HMMER may well report more than one domain per
target sequence.

For single domain proteins, the per-sequence and per-domain scores are
identical. For multi-domain proteins, the per-sequence score is {\em
roughly} the sum of the individual per-domain scores. 

You can specify cutoffs for per-sequence and/or per-domain scores
and/or E-values when you use \prog{hmmsearch} or \prog{hmmpfam}.  For
a specific sensible use of such cutoffs, read about how the Pfam
TC/NC/GA cutoffs are set and used (below).

There are some technical points that may be of interest here;
especially for developers trying to mirror HMMER raw scores.

First, HMMER will always report a score for the best domain it finds
in the target sequence, even if that hit is a negative raw score. Why?
If you look at the Plan 7 model architecture, you see that a target
sequence must pass through the model at least one time. However, each
subsequent domain must have a positive bit score to be identified.
(Why? Because HMMER finds the best-scoring alignment of the sequence
to the HMM; if a domain has a negative bit score, HMMER finds a better
scoring alignment by using the ``nonhomologous'' N, C, or J states to
account for the residues in that domain.) Therefore HMMER will never
find more than one domain in a target sequence that has a negative
per-sequence score \textit{even if there are additional domains with
significant E-values}. Thus, counterintuitively, you can find cases
where you find additional significant domains by searching against a
sequence that you've cut into pieces. This is not a bug; it's a
limitation of the approach.

Second, HMMER actually implements two null models, and by default, raw
scores take both models into account. The second one is an ad hoc null
model, called ``null2'', which compensates for target sequences with
biased composition. Biased composition filters are important in any
database searching application. The BLAST filters, SEG, XNU, and DUST,
are aggressive filters that completely X out part of a
sequence. Sometimes, this filters out real homologues, if you happen
to be searching with a ``real'' biased composition query (collagens
are a great example -- BLAST has a hard time finding collagen
homologues unless you switch the filters off.) The inner workings of
the HMMER null2 model are too detailed and too in flux to describe in
detail here, but suffice it to say that it exists, and it's the reason
that per-domain scores don't sum up to the per-sequence score.  

There are certain inconsistencies (not to say ``bugs'') known with the
current null2 approach, and the code is also now known to be
incompatible with the design of some hardware accelerators, so
developers should expect the relevant code to change without notice.

\subsection{In more detail: What HMMER E-values mean}

The E-value of a hit is the expected number of false positives with
scores as good as this hit in the search you did.

Unlike the raw score, the E-value is dependent on the size of the
database you search. If you detect a hit with an E-value of 0.1 in a
search of a sequence database with 100,000 sequences, then you happen
to re-score the sequence all by itself, you will get an E-value
100,000 times better. The E-value is quite literally the expected
number of false positives at this raw score; the larger the database
you search, the greater the number of expected false positives.

This behavior is shared with BLAST and FASTA P-value and E-values, so
it will not be unfamiliar to most users. However, it can cause
consternation: one phenomenon is that a hit that is marginally
significant this year may no longer be significant next year, when the
database is twice as large. Some people argue that this is wrong, but
it's just something you have to deal with; that's statistics for you.

HMMER has two ways of calculating E-values. One way is inaccurate but
analytical (fast); the other way is more accurate but empirical
(slow).

If your HMM has not been calibrated with \prog{hmmcalibrate}, HMMER
uses the analytic calculation. This is a conservative calculation,
meaning that the ``true'' E-value will be lower; sometimes much lower.
The calculation is essentially the same as that given in
\cite{Barrett97}.

It is highly recommended that you calibrate HMMs with
\prog{hmmcalibrate}. \prog{hmmcalibrate} writes two parameters
into your HMM file on a line labeled ``EVD'': these parameters are the
$\mu$ (location) and $\lambda$ (scale) parameters of an extreme value
distribution (EVD) that best fits a histogram of scores calculated on
randomly generated sequences of about the same length and residue
composition as SWISS-PROT. You only need to do this calibration once
for a given HMM. All the Pfam HMMs come pre-calibrated.

Now, there's good news and bad news about extreme value distributions.

Frst the good news. The extreme value distribution fitting is done
with a rather robust and personally satisfying chunk of maximum
likelihood code (see \prog{histogram.c} in the codebase). This code is
portable enough that it is part of the BioPerl distribution, as an XS
C module. The ML approach was suggested to me by Stephen Altschul, who
directed me to a lovely textbook by Lawless \cite{Lawless82}. See the
code for details. Any EVD fitting code that is relying on linear
regression fits to log-log plots is bound to be less accurate, judging
from my tests. (This currently includes FASTA and PFSCAN, to the best
of my knowledge.)

However, the down side is that most profile HMM scores don't fit the
extreme value distribution well. Fully local alignments (models built
with \prog{hmmbuild -f} or \prog{hmmbuild -s} fit the EVD fairly well,
as expected for Smith/Waterman style alignments, for the same reasons
that gapped BLAST or FASTA alignment scores have been shown to be
approximately EVD-distributed. By default, though, \prog{hmmbuild}
makes models for ``glocal'' alignments which are global with respect
to the model, and multi-hit-local with respect to the sequence.  Also
called ``profile scores'' by Waterman, this sort of alignment scores
are known not to be EVD-distributed \cite{GoldsteinWaterman94}.

Nonetheless, empirically, the tail of the distribution around E=1 is
falling off more or less like an EVD, and this is the region of
interest. HMMER does a ``censored EVD fit'' from the peak of the
distribution down to the right tail, and does not include the data to
the left of the peak in its fit. This produces a reasonable fit in the
important region of the scores. Empirically, HMMER E-values tend to be
accurate in the critical region (E~1), despite the lack of
mathematical foundation.

The end of \prog{hmmsearch} output shows you the observed histogram,
and (if the model is calibrated) an expected curve according to the
EVD fit. You should observe that the relevant tail (around E=1) is
more or less well fitted. The bulk of the distribution, particularly
at lower scores, is usually poorly fitted, but this is not the area of
interest.

\subsection{In more detail: What Pfam TC/NC/GA cutoffs mean}

When a Pfam model is built, the Pfam team keeps track of per-sequence
and per-domain scores of every sequence in a large nonredundant
database. They record three types of score cutoffs on Pfam HMM files:

\begin{wideitem}
\item[GA (gathering cutoffs)]: the scores used as cutoffs in
constructing Pfam. All domains that are in a sequence satisfying the
GA1 per-sequence score, and which themselves satisfy the GA2
per-domain score, will be included in the Pfam ``full alignment''.

\item[TC (trusted cutoffs)]: the scores of the lowest-scoring hit(s)
that were included as true member(s) of the Pfam family. The
per-domain TC2 score is the score of the lowest scoring domain
\textit{in a sequence with a per-sequence score over the TC1 cutoff};
therefore, the TC1 and TC2 scores could conceivably come from
different targets. Hits above these scores are ``within'' the Pfam
family and almost certainly members.

\item[NC (noise cutoffs)]: the scores of the highest-scoring
hit(s) that were \textit{not} included as true members of the Pfam
family, because they were considered to be the top of the noise.
Calculated analogously to TC1 and TC2. Hits above the NC cutoff are
above the top scoring noise in the Pfam NR database search, so are
likely homologues, but not as trustworthy as hits over the GA or TC
cutoffs.
\end{wideitem}

In order of increasing conservativeness, the cutoffs rank: NC, GA, and
TC.

The GA cutoffs, being the actual cutoffs used in constructing Pfam,
are a very good choice to use to collate large-scale automated data,
like counting protein family members in a sequenced genome.

The TC and NC cutoffs are less useful, and only really there as
documentation of Pfam construction. In general, the TC cutoffs would
be extremely conservative cutoffs to use in a database search, more
conservative than GA. The NC cutoffs are less conservative than GA.

Why use GA (or the other cutoffs) instead of the E-value? Pfam
artificially imposes a ``flat'', nonhierarchical structure on protein
sequence space.  Pfam asserts that no Pfam family is related to any
other Pfam family. This is obvious nonsense: many Pfam families are in
fact homologous.  The different 7-TM (G-protein coupled receptor)
families are one example; the different GTPase families are
another. HMMER often detect significant relationships between families
that Pfam chooses to suppress. In these cases, the Pfam GA cutoff will
be elevated to artifically separate two homologous but distantly
related subgroups of the same structural superfamily. (On a related
note, this means that if you happen to observe that Pfam ``doesn't
recognize the similarity between family X and Y'', but your method Z
does, please don't go writing a paper that claims HMMs don't detect
the similarity until you've done the experiment with HMMs instead of
just looking at Pfam classifications. Yes, such paper(s) have been
written. Sigh.)

The mechanism that HMMER uses to incorporate up these cutoffs is
general: Stockholm format multiple sequence alignments can carry
appropriate TC, NC, and GA markup lines. This means that you can use a
Pfam-like cutoff system if you like, just by adding the appropriate
Stockholm markup to your collection of alignments. When these numbers
are available in the HMM, HMMER search programs provide options
(\prog{--cut_ga}, etc.) for setting search cutoffs to GA, TC, or NC
automatically.
