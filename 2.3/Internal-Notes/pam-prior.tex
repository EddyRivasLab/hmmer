\documentstyle{article}
\begin{document}

\section {Evolving HMMs}

An HMM is a statistical model of a set of example (training)
sequences. One would think that it can only model sequences like those
it's seen already -- it's only as good as the training data.  So, if
there's a more distant sequence, dissimilar from all the training data
but sharing recognizably the same structure, our HMM won't recognize
it. This section is about attempts to push further out into the
twilight zone of distant sequence similarities, by using extra prior
information about protein structures to ``reach out'' in specific
sensible directions in sequence space.

The basic idea is to start with an HMM and evolve it (change its
statistics) according to a specific evolutionary model. The HMM loses
information and smooths out, but with position- and residue
type-specific rates. We'll do this with {\em substitution matrices},
related to the Dayhoff PAM matrices, which give the probability that
one amino acid will be substituted for another in two sequences
separated by some evolutionary time. 

In the simplest incarnation, we will use a single substitution matrix
for all positions and a single time parameter.  In the more
complicated incarnations, we will use position-specific substitution
matrices (assigned based on the position's structural environment) and
position-specific time scaling constants (assigned based on the
apparent conservation pressure at each site).

Two special cases of our theory are of interest.  The single matrix,
single time form is a Gribskov/McLachlan/Eisenberg profile (except our
math is more firmly based in probability theory). The mixture of
environment-specific substitution matrices, applied for infinite time,
gives a Bowie/Luthy/Eisenberg 3D structural profile. Our interest is
in a smooth mixture of these two apparently different methods.

\subsection {On substitution matrices}

Ready for a big assumption? Let's assume that our sequences are
generated from a single root sequence, in a ``star'' instead of a
tree, and that all of them are the same evolutionary distance from the
root. There, I said it. I'm throwing out the fact that our data are
related by a tree and the fact that evolutionary rates may differ on
different branches. We'll get back some of the tree's effects via
weighting corrections, but it's important to realize that this is our
model of evolution.

A substitution matrix $S$ gives us the conditional probability $P(c |
r)$, i.e.\ the probability that the root's amino acid $r$ becomes a
child's amino acid $c$.

A different substitution matrix is used for each time (evolutionary
distance). We can keep individual matrices, or more conveniently, we
can extrapolate to long-distance matrices by exponentiating a
short-distance matrix. (To extrapolate to arbitrary rather than
integral distances, I guess you'd need to calculate eigenvalues and
eigenvectors for the substitution matrix, raise the diagonal matrix of
eigenvalues to the arbitrary power, and use the eigenvectors to
convert back to a substitution matrix. Hopefully, the NCBI program PAM
will do all this for me.)

\subsubsection {Converting PAM/BLOSUM to conditional probabilities}

A Dayhoff PAM matrix (or a Henikoff BLOSUM matrix, or any of the
matrices used for scoring alignments) is *not* in this form.  The
scoring matrices are in log-odds form \cite{Altschul91}.  We can
derive the conditional probability from a PAM matrix if we have to.

From Altschul (1991), we know that the PAM matrix entry is the log of
the probability of seeing amino acids $i$ and $j$ in an alignment at
some evolutionary distance, divided by the probability that they would
be appear aligned by chance (taken to be the product of the
frequencies that each appears, $f_{i}$ and $f_{j}$):

\begin{equation}
 M_{ij} = \log \frac{q_{ij}}{f_{i} f_{j}}
\end{equation}

and from the laws of conditional probability, we know that:

\begin{equation}
 P(B \mid A) = \frac{P(B \cap A)}{P(A)}
\end{equation}

We know the overall frequencies of amino acids $f_{i}$ and $f_{j}$
so we can calculate $q_{ij}$:

\begin{equation}
 q_{ij} = f_{i} f_{j} e^{M_{ij}}
\end{equation}

and $q_{ij}$ is $P(A \cap B)$ as far as our conditional probability
equation goes, so:

\begin{equation}
 P(c | r) = S_{cr} = f_{c} e^{M_{cr}}
\end{equation}

\subsubsection {Estimating the root sequence from count data}

We don't know the sequence of the root, $R$. We can, however, derive a
probability distribution for what it might be there at every position,
given a substitution matrix and an observed ``count vector'' $n_{c}$
for each column which gives the number of times we observed amino acid
$c$ in that column of the multiple sequence alignment.

The likelihood that we would see the count vector given that the root
had some sequence $r$ is:

\begin{equation}
P(\vec{n} \mid r) = \prod_{c = A}^{Y} (P(c \mid r))^{n_{c}} 
\end{equation}

where $P(c \mid r)$ is the entry $S_{cr}$ in the substitution matrix.

The sequence probability distribution at the root is given
by inverting this likelihood using Bayes' theorem:

\begin{equation}
P(r \mid \vec{n}) = \frac{ (\prod_{c = A}^{Y} (P(c \mid r))^{n_{c}} ) P(r) }
                         { Z }
\end{equation}

where Z is the appropriate normalization factor, obtained by summing
the numerators for all twenty possible amino acids $r$, and $P(r)$ is
the prior expectation of $r$ -- which in the simplest case would be
the average frequencies of the amino acids. (A mixture matrix model
might include environment-specific priors.)

\subsubsection { Calculating the likelihood of data given a matrix }

This is a basic operation which we'll have to use over and over again.
We have a matrix $S_{cr}$ which gives us the conditional probabilities
$P(c|r)$, and we have an observed ``count vector'' $n_{i}$ which gives
the number of times we observed amino acid $i$ in some column of the
multiple sequence alignment. We also have $P(r)$, the prior
probability distribution on the symbols of the root.

Then we use a calculation like that we used above to calculate the
likelihood of the observed counts data given the matrix:

\begin {eqnarray}
P(\vec{n} \mid S) & = & P(r) P(\vec{n} \mid S,r) \\
                  & = & \sum_{r=A}^{Y}  P(r) \prod_{c = A}^{Y} (P(c \mid r))^{n_{c}} 
\end {eqnarray}

We can use this calculation to compare different matrices on the same
count data, and find the optimal (maximum likelihood) choice of matrix
and time.

\end{document}


